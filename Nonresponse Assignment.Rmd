---
title: "Sampling and Estimation | Nonresponse Assignment"
author: "Marco Ramljak"
output:
  word_document:
    toc: yes
  html_document:
    code_folding: show
    theme: lumen
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

These assignments are based on for the Non-response course within the EMOS module. The following solution methods and interpretations are based on the slides by Barry Schouten and the Chapter 38 - Nonresponse error: detection and correction in the Handbook of Survey Methodology, also based on Barry Schouten and Jelke Bethlehem.

```{r message=FALSE}
library(tidyverse)
library(haven)
library(sjmisc)
library(broom)
library(knitr)
library(kableExtra)

dat.raw <- read_spss("Sampling and Estimation/EMOS - Sampling and Estimation -exercises Nonresponse/gps.sav")
dat <- dat.raw %>% 
  mutate_all(as_factor) %>% 
  sjlabelled::copy_labels(dat.raw)

```

# Assignment 1: Response rate and target variables

```{r}
frq(dat$Result) # the response rate in the survey based on the variable Result is 58.69 % (cat 1 Response)

frq(dat$HasPC) # of the 18792 people with valid responses, 57.38% own a PC
frq(dat$OwnHouse) # of the 18792 people with valid responses, 62.52% own a house


# custom function that calculates the 95% confidence interval for proportions
con_int <- function(x, y, z) {
  object <- x %>% 
    filter((!!as.name(y)) == z) %>% 
    select(all_of(y))
  cat <- object %>% 
    deframe()
  n <- x %>%
    filter(!is.na((!!as.name(y)))) %>%
    select(all_of(y)) %>%
    deframe()
  p <- length(cat) / length(n)

  std.error <- sqrt((p * (1 - p)) / length(n))

  upper <- p + 1.96 * std.error
  lower <- p - 1.96 * std.error
  return(
    c("Variable" = names(object), "Lower bound" = round(lower, 4) * 100, 
      "Upper bound" = round(upper, 4) * 100))
}

con_int(x = dat, y = "HasPC", z = 1)
con_int(x = dat, y = "OwnHouse", z = 1)

```

In order to calculate the worst case lower and upper limits of the sample percentages concerning the two variable one needs to add the respective non-response percentage to each the Yes and No percentages. This is possible with the values from the raw percentages above (column 4):

| Variable       |  Yes  |     No|
|----------------|:-----:|------:|
| HasPC Upper    | 74.98 |  25.02|
| HasPC Lower    | 33.67 |  46.33|
| OwnHouse Upper | 78.00 |  22.00|
| OwnHouse Lower | 36.69 |  63.31|

# Assignment 2: Exploring the nonresponse

```{r}
# helper to include auxiliary variables
not.aux <- c("HasPC", "OwnHouse", "Employed", "Respons1", "Response", "Contact", "Able")
dat.new <- dat %>% 
  select(-all_of(not.aux)) %>% 
  mutate(Result.dummy = case_when(Result == "Response" ~ "Responded", # define if people responded or not
                                  TRUE ~ "Missing"))

# execute chi square tests for all mentioned variables, save the statistic-value and calculate cramers V to differntiate between the variables.
chi.result <- dat.new %>% 
  map(~chisq.test(., dat.new$Result.dummy)) %>% 
  map_df(glance, .id = "variables") %>% 
  mutate(cramersV = sqrt(statistic / 32019))

chi.result %>% 
  filter(!variables %in% c("Result", "Result.dummy")) %>% 
  ggplot() +
  geom_bar(aes(x = reorder(variables, cramersV), y = cramersV), stat = "identity") +
  coord_flip()

```

All variables in 2a had a significant p-value which emphasizes the importance of all auxiliary variables in the context of non-response bias. This means that the respondents differ in auxiliary variable characteristics significantly from non-respondents (Contingency tables are not represented due to space issues). However, no ranking (strongest candidates) should be applied only based on p-values, therefore the CramersV statistic is calculated.\
The graph presents the variables in descending order based on the CramersV value, i.e. the strength of their association with the response indicator. `Region`, `Urban` and `Phone` have the highest values, meaning a high dispersion of respondents and non-respondents within these variables, while there is almost no non-response bias in the context of `gender`.

# Assignment 4: Exploring representativity

```{r}
# Modelling the composition of non-response by means of a logistic regression model
# Original response
mod.4a <- glm(Response ~ Region + Urban + Phone + HouseVal + Ethnic +  
                HHType, family = "binomial", data = dat)
resp.prop1 <- mod.4a %>% 
  augment(type.predict = "response")


# R-indicator
(R.ind.1 <- 1 - 2 * sd(resp.prop1$.fitted)) 
# Coefficient of Variation
(CV.1 <- (1 - R.ind.1) / (2 * mean(resp.prop1$.fitted)))


# Response propensities after one month
mod.4b <- glm(Respons1 ~ Region + Urban + Phone + HouseVal + Ethnic +  
                HHType, family = "binomial", data = dat)
resp.prop2 <- mod.4b %>% 
  augment(type.predict = "response")

# R-indicator
(R.ind.2 <- 1 - 2 * sd(resp.prop2$.fitted)) 
# Coefficient of Variation
(CV.2 <- (1 - R.ind.2) / (2 * mean(resp.prop2$.fitted)))
```

The Propensity score is the conditional probability that a person with characteristics X responds in the survey. The R-indicator and the CV are standardized measures that make it easy to compare non-response error between surveys. The R-indicator for X is defined as the standard deviation of the response propensities and transformed to be between 0 and 1. 1 indicating that the response is fully representative because all propensities are equal, and 0 indicating the largest possible deviation from a representative response. The R-indicator for the first model (after two months) is around 0.778 and for the second model (after one month) 0.812, meaning that the general representativity is basically at a high level but dropped in the second month compared to the first month.\
The CV represents the response propensities and their maximal standardized bias for all variables that are linear combinations of the components of X. It therefore represents an upper bound of the possible non-response bias, for the auxiliary variables within (and only for) X. In this case the CV decreases because its value after the second month was 0.189 and for the first month it was 0.202.

# Assignment 5: Nonresponse adjustment using propensity weighting

```{r}
# inverse propensity weighting weight
dat.red <- dat %>% 
  select(HasPC, OwnHouse)

resp.prop.5a <- resp.prop1 %>% 
  bind_cols(dat.red) %>% # bind the target variables to the object with the predicted values
  mutate(inv.prop.weight = 1 / .fitted)


frq(resp.prop.5a$HasPC, weights = resp.prop.5a$inv.prop.weight) 
frq(resp.prop.5a$OwnHouse, weights = resp.prop.5a$inv.prop.weight)
```

For 5a the inverse propensity weights are calculated and with these new weights one can compute new estimates for the target variables. As one can see the proportions are adjusted now by 1.83% in favor of not owning a PC and by 3.91% in favor of not owning a house.

```{r}
# Propensity class weighting with poststratification
resp.prop.5b <- resp.prop.5a %>% 
  mutate(strata.weight.5 = cut(.fitted, 5),
         strata.weight.20 = cut(.fitted, 20))

# Visualising the response probability bins (inv.propensity weight)
resp.prop.5b %>% 
  ggplot() +
  geom_jitter(aes(x = .fitted, y = strata.weight.5))

resp.prop.5b %>% 
  ggplot() +
  geom_jitter(aes(x = .fitted, y  = strata.weight.20))

  
# Poststratified estimator for 5 strata
(est.strat.class.5 <- resp.prop.5b %>% 
  group_by(strata.weight.5) %>% 
  summarise(w.str = mean(inv.prop.weight),
            n.str = n(),
            HasPC.prop = sum(as.numeric(HasPC) / n(), na.rm = T),
            OwnHouse.prop = sum(as.numeric(OwnHouse) / n(), na.rm = T)) %>% 
  summarise(HasPC.prop5 = sum(HasPC.prop * w.str) / sum(w.str) * 100,
            OwnHouse.prop5 = sum(OwnHouse.prop * w.str) / sum(w.str) * 100))

# Poststratified estimator for 20 strata
(est.strat.class.20 <- resp.prop.5b %>% 
  group_by(strata.weight.20) %>% 
  summarise(w.str = mean(inv.prop.weight),
            n.str = n(),
            HasPC.prop = sum(as.numeric(HasPC) / n(), na.rm = T),
            OwnHouse.prop = sum(as.numeric(OwnHouse) / n(), na.rm = T)) %>% 
  summarise(HasPC.prop20 = sum(HasPC.prop * w.str) / sum(w.str) * 100,
            OwnHouse.prop20 = sum(OwnHouse.prop * w.str) / sum(w.str) * 100))

```

For 5b propensity class weighting with post-stratification is applied. The idea is to form strata based on similar or even equal inclusion probabilities. The inclusion probabilities are modeled and are represented by the inverse propensity weights from 5a. Then a weighted stratified population estimate can be computed based on the strata defined. Two options are presented, one option models the estimate based on 5 strata and one based on 20 strata. The two plots show the bins and ranges of these strata for each option, the points represent each observation within each strata (jitter function applied).

The adjustment with 5 strata compared to the unadjusted values from 1b are 2.04 in favor of owning a PC and 0.29 in favor of owning a house. In the case for 20 strata it is 5.39 in favor of not owning a PC and 7.89 in favor of not owning a house. However, I believe I made some wrong calculations for these postratified class estimators. They should have more similar results to 5a, not such a large range.

# Assignment 7: Adaptive survey design

In order to reduce the response propensities in month 1 one should scrutinize groups that have on average large propensities (large differences within a group in response). One can use the ChiÂ² analysis from beforehand for this as an approximation, as one can see that there are indeed differences within groups. Based on this I would propose to target people by phone in urban areas as one can infer from the Region variable and the Urban variable. I would follow the same strategy if the budget would be limited because one can see from the proportions listed below (grouped by Urban) that more than 58 per cent of observations living in strongly urbanized areas have a telephone number which can be accessed.

```{r}
resp.prop.5a %>% 
  group_by(Urban) %>% 
  frq(Phone)
```
